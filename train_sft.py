import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset

#VRAMä¸è¶³ã§å®Ÿè¡Œã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
# VRAM ã‚’ãƒªã‚»ãƒƒãƒˆ
torch.cuda.empty_cache()

# GPU ã®èªè­˜ãƒ­ã‚°ã‚’å‡ºåŠ›
print("ğŸš€ PyTorch ãƒãƒ¼ã‚¸ãƒ§ãƒ³:", torch.__version__)
print("ğŸ” CUDA åˆ©ç”¨å¯èƒ½:", torch.cuda.is_available())

if torch.cuda.is_available():
    device = "cuda"
    print("ä½¿ç”¨ä¸­ã® GPU:", torch.cuda.get_device_name(torch.cuda.current_device()))
    print("ä½¿ç”¨å¯èƒ½ãª VRAM:", round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2), "GB")
else:
    device = "cpu"
    print("CUDA ãŒä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚CPU ã§å®Ÿè¡Œã—ã¾ã™ã€‚")

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
model_name = "elyza/Llama-3-ELYZA-JP-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # ã‚¨ãƒ©ãƒ¼ä¿®æ­£

# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆVRAMç¯€ç´„ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16  # FP16 â†’ BF16 ã«å¤‰æ›´
).to(device)

# ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–
model.gradient_checkpointing_enable()

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ï¼ˆJSONLå½¢å¼ï¼‰
dataset = load_dataset("json", data_files={"train": "data.jsonl"})

# ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼‰
def preprocess_function(examples):
    inputs = ["ãƒ„ãƒ³ãƒ‡ãƒ¬ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n\nQ: " + q + "\nA:" for q in examples["input"]]
    outputs = [a for a in examples["output"]]

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    model_inputs = tokenizer(inputs, padding="max_length", truncation=True, max_length=512)
    labels = tokenizer(outputs, padding="max_length", truncation=True, max_length=512)

    # `labels` ã®è¿½åŠ ï¼ˆæ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ã†ï¼‰
    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

# ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
tokenized_dataset = dataset.map(preprocess_function, batched=True)

# å­¦ç¿’è¨­å®šï¼ˆVRAMç¯€ç´„ï¼‰
training_args = TrainingArguments(
    output_dir="./sft-model",
    eval_strategy="no",
    save_strategy="epoch",
    per_device_train_batch_size=2,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,  # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ç´¯ç©å‹¾é…ã‚’ä½¿ç”¨
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2,
    bf16=True  # `fp16` ã§ã¯ãªã `bf16` ã«å¤‰æ›´ï¼ˆVRAMç¯€ç´„ï¼‰
)

# Trainer ã®è¨­å®š
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"]
)

# VRAMæƒ…å ±ã‚’å‡ºåŠ›
if device == "cuda":
    print("ğŸ” å­¦ç¿’å‰ã® VRAM ä½¿ç”¨é‡:", round(torch.cuda.memory_allocated() / 1024**3, 2), "GB")

# å­¦ç¿’ã®å®Ÿè¡Œ
trainer.train()

# å­¦ç¿’å¾Œã® VRAMæƒ…å ±ã‚’å‡ºåŠ›
if device == "cuda":
    print("å­¦ç¿’å¾Œã® VRAM ä½¿ç”¨é‡:", round(torch.cuda.memory_allocated() / 1024**3, 2), "GB")

# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
model.save_pretrained("./sft-model")
tokenizer.save_pretrained("./sft-model")
