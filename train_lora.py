import os
import torch
from dotenv import load_dotenv
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer

# `.env` ã®èª­ã¿è¾¼ã¿
load_dotenv()
MODEL_NAME = os.getenv("MODEL_NAME", "elyza/Llama-3-ELYZA-JP-8B")
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

# ãƒ‡ãƒã‚¤ã‚¹ã®ç¢º
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HUGGINGFACE_TOKEN)
tokenizer.pad_token = tokenizer.eos_token  # `pad_token` ã‚’ `eos_token` ã«è¨­å®š

# `to_empty()` ã‚’é©ç”¨ã—ã¦ `meta` ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰ GPU ã¸ç§»å‹•
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    token=HUGGINGFACE_TOKEN,
    torch_dtype=torch.float16,
    device_map="auto"
)
model.to_empty(device=device)

# LoRA è¨­å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰
lora_config = LoraConfig(
    r=8,                 # å­¦ç¿’å¯èƒ½ãªè¡Œåˆ—ã®æ¬¡å…ƒæ•° (r=8ã§æƒãˆã‚‹)
    lora_alpha=32,        # LoRA ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
    lora_dropout=0.1,     # æ±åŒ–æ€§èƒ½å‘ä¸Šã®ãŸã‚ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "down_proj", "up_proj"] # FFNå±¤ã‚‚LoRAé©ç”¨
)

# LoRA ã®é©ç”¨
model = get_peft_model(model, lora_config)
model.train()

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
dataset = load_dataset("json", data_files={"train": "tsundere_responses.jsonl"})["train"]

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ãƒ„ãƒ³ãƒ‡ãƒ¬AIã®è‡ªç„¶ãªä¼šè©±ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
def generate_prompt(data_point):
    return f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {data_point['input']}\nãƒ„ãƒ³ãƒ‡ãƒ¬AI: {data_point['output']}"

dataset = dataset.map(lambda x: {"text": generate_prompt(x)})

# å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
training_arguments = TrainingArguments(
    output_dir="./train_logs",
    per_device_train_batch_size=6,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã—ã¦å®‰å®šåŒ–
    gradient_accumulation_steps=2,
    optim="adamw_torch",
    learning_rate=5e-5,  # ä½ã‚ã®å­¦ç¿’ç‡ã§å®‰å®š
    max_steps=300,  # ã—ã£ã‹ã‚Šå­¦ç¿’
    logging_steps=50,
    save_steps=100,
    fp16=True,
    report_to="none"
)

# LoRA å­¦ç¿’ã®è¨­å®š
trainer = SFTTrainer(
    model=model,  
    tokenizer=tokenizer,  
    train_dataset=dataset,  
    dataset_text_field="text",
    peft_config=lora_config,  
    args=training_arguments,  
    max_seq_length=1024,
    packing=False
)

# å­¦ç¿’é–‹å§‹
trainer.train()

# å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
trainer.model.save_pretrained("./lora-tundere-ai")
tokenizer.save_pretrained("./lora-tundere-ai")

print("LoRA ã®é‡ã¿ã‚’ `./lora-tundere-ai` ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
